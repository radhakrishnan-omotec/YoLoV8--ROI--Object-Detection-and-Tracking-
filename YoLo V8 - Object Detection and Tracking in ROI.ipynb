{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoLo V8 - Object Detection and Tracking in ROI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detecion \n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display image and videos\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import urllib.request \n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Object Detection and Counting project in a ROI based on the Yolo V8 Model.\n",
    "----------------------\n",
    " **The objectives of the project are:**\n",
    "- To detect objects that are passing in a Region of interest (ROI) \n",
    "- Track each individual with a unique ID in the ROI \n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video  path for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMOLP091\\Documents\\GitHub\\Yolov8-Counting-People-in-Queue\\working\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\\\Users\\\\OMOLP091\\\\Documents\\\\GitHub\\\\Yolov8-Counting-People-in-Queue\\\\working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video  path for experiment\n",
    "path_zip = 'https://github.com/freedomwebtech/roiinyolo/raw/main/vid1.zip' # credits to the github repo for the video\n",
    "urllib.request.urlretrieve(path_zip, \"vid1.zip\")\n",
    "shutil.unpack_archive('C:\\\\Users\\\\OMOLP091\\\\Documents\\\\GitHub\\\\Yolov8-Counting-People-in-Queue\\\\working\\\\vid1.zip', 'C:\\\\Users\\\\OMOLP091\\\\Documents\\\\GitHub\\\\Yolov8-Counting-People-in-Queue\\\\working\\\\')\n",
    "\n",
    "path = 'C:\\\\Users\\\\OMOLP091\\\\Documents\\\\GitHub\\\\Yolov8-Counting-People-in-Queue\\\\working\\\\vid1.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A : loading a YOLO model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt to yolov8x.pt...\n",
      "100%|██████████| 131M/131M [00:04<00:00, 28.1MB/s] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading a YOLO model \n",
    "model = YOLO('yolov8x.pt')\n",
    "\n",
    "#geting names from classes\n",
    "dict_classes = model.model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def risize_frame(frame, scale_percent):\n",
    "    \"\"\"Function to resize an image in a percent scale\"\"\"\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    # resize image\n",
    "    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "\n",
    "\n",
    "def filter_tracks(centers, patience):\n",
    "    \"\"\"Function to filter track history\"\"\"\n",
    "    filter_dict = {}\n",
    "    for k, i in centers.items():\n",
    "        d_frames = i.items()\n",
    "        filter_dict[k] = dict(list(d_frames)[-patience:])\n",
    "\n",
    "    return filter_dict\n",
    "\n",
    "\n",
    "def update_tracking(centers_old,obj_center, thr_centers, lastKey, frame, frame_max):\n",
    "    \"\"\"Function to update track of objects\"\"\"\n",
    "    is_new = 0\n",
    "    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) for k, center in centers_old.items()]\n",
    "    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]\n",
    "    # Calculating distance from existing centers points\n",
    "    previous_pos = [(k,obj_center) for k,centers in lastpos if (np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers)]\n",
    "    # if distance less than a threshold, it will update its positions\n",
    "    if previous_pos:\n",
    "        id_obj = previous_pos[0][0]\n",
    "        centers_old[id_obj][frame] = obj_center\n",
    "    \n",
    "    # Else a new ID will be set to the given object\n",
    "    else:\n",
    "        if lastKey:\n",
    "            last = lastKey.split('D')[1]\n",
    "            id_obj = 'ID' + str(int(last)+1)\n",
    "        else:\n",
    "            id_obj = 'ID0'\n",
    "            \n",
    "        is_new = 1\n",
    "        centers_old[id_obj] = {frame:obj_center}\n",
    "        lastKey = list(centers_old.keys())[-1]\n",
    "\n",
    "    \n",
    "    return centers_old, id_obj, is_new, lastKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B : Detecting Object in ROI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "# ### Configurations\n",
    "#Verbose during prediction\n",
    "verbose = False\n",
    "# Scaling percentage of original frame\n",
    "scale_percent = 100\n",
    "# model confidence level\n",
    "conf_level = 0.8\n",
    "# Threshold of centers ( old\\new)\n",
    "thr_centers = 20\n",
    "#Number of max frames to consider a object lost \n",
    "frame_max = 5\n",
    "# Number of max tracked centers stored \n",
    "patience = 100\n",
    "# ROI area color transparency\n",
    "alpha = 0.1\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading video with cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH :: C:\\Users\\OMOLP091\\Documents\\GitHub\\Yolov8-Counting-People-in-Queue\\working\\vid1.mp4\n",
      "[INFO] - Verbose during Prediction: False\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------\n",
    "# Reading video with cv2\n",
    "print(\"PATH ::\", path)\n",
    "video = cv2.VideoCapture(path)\n",
    "\n",
    "# Objects to detect Yolo\n",
    "class_IDS = [0] \n",
    "# Auxiliary variables\n",
    "centers_old = {}\n",
    "\n",
    "obj_id = 0 \n",
    "end = []\n",
    "frames_list = []\n",
    "count_p = 0\n",
    "lastKey = ''\n",
    "print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original informations of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Original Dim:  (1920, 1080)\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------\n",
    "# # Original informations of video\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print('[INFO] - Original Dim: ', (width, height))\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Video for better performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Non Scaled:  (1920, 1080)\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------\n",
    "# # Scaling Video for better performance \n",
    "if scale_percent != 100:\n",
    "    print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    \n",
    "else:\n",
    "    print('[INFO] - Non Scaled: ', (width, height))\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "### Video output ####\n",
    "video_name = 'result.mp4'\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "output_video = cv2.VideoWriter(tmp_output_path, \n",
    "                               cv2.VideoWriter_fourcc(*VIDEO_CODEC), \n",
    "                               fps, (width, height))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d33eb3c37d490fb67d38b528c6d6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid CUDA 'device=0' requested, use 'device=cpu' or pass valid CUDA device(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension Scaled(frame): \u001b[39m\u001b[38;5;124m'\u001b[39m, (frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Getting predictions\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_IDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m boxes   \u001b[38;5;241m=\u001b[39m y_hat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\OMOLP091\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OMOLP091\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:143\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[1;34m(self, source, stream, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPredictorClass(overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, overrides)\n",
      "File \u001b[1;32mc:\\Users\\OMOLP091\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:233\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[1;32m--> 233\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m     model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# half precision only supported on CUDA\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OMOLP091\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\utils\\torch_utils.py:75\u001b[0m, in \u001b[0;36mselect_device\u001b[1;34m(device, batch, newline)\u001b[0m\n\u001b[0;32m     73\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m device  \u001b[38;5;66;03m# set environment variable - must be before assert is_available()\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(device\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))):\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requested, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or pass valid CUDA device(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     devices \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# range(torch.cuda.device_count())  # i.e. 0,1,6,7\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid CUDA 'device=0' requested, use 'device=cpu' or pass valid CUDA device(s)"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------\n",
    "# Executing Recognition \n",
    "for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):\n",
    "    \n",
    "    # reading frame from video\n",
    "    _, frame = video.read()\n",
    "    \n",
    "    #Applying resizing of read frame\n",
    "    frame  = risize_frame(frame, scale_percent)\n",
    "#     frame  = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    area_roi = [np.array([ (1250, 400),(750,400),(700,800) ,(1200,800)], np.int32)]\n",
    "    ROI = frame[390:800, 700:1300]\n",
    "\n",
    "  \n",
    "    if verbose:\n",
    "        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Getting predictions\n",
    "    y_hat = model.predict(ROI, conf = conf_level, classes = class_IDS, device = 0, verbose = False)\n",
    "    \n",
    "    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\n",
    "    boxes   = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "    conf    = y_hat[0].boxes.conf.cpu().numpy()\n",
    "    classes = y_hat[0].boxes.cls.cpu().numpy() \n",
    "    \n",
    "    # Storing the above information in a dataframe\n",
    "    positions_frame = pd.DataFrame(y_hat[0].cpu().numpy().boxes.boxes, columns = ['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "    \n",
    "    #Translating the numeric class labels to text\n",
    "    labels = [dict_classes[i] for i in classes]\n",
    "    \n",
    "    \n",
    "    # For each object, draw the bounding-box and counting each one the pass thought the ROI area\n",
    "    for ix, row in enumerate(positions_frame.iterrows()):\n",
    "        # Getting the coordinates of each vehicle (row)\n",
    "        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')\n",
    "        \n",
    "        # Calculating the center of the bounding-box\n",
    "        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)\n",
    "        \n",
    "        #Updating the tracking for each object\n",
    "        centers_old, id_obj, is_new, lastKey = update_tracking(centers_old, (center_x, center_y), thr_centers, lastKey, i, frame_max)\n",
    "        \n",
    "\n",
    "        #Updating object in roi\n",
    "        count_p+=is_new\n",
    "        \n",
    "        # drawing center and bounding-box in the given frame \n",
    "        cv2.rectangle(ROI, (xmin, ymin), (xmax, ymax), (0,0,255), 2) # box\n",
    "        for center_x,center_y in centers_old[id_obj].values():\n",
    "            cv2.circle(ROI, (center_x,center_y), 5,(0,0,255),-1) # center of box\n",
    "        \n",
    "        #Drawing above the bounding-box the name of class recognized.\n",
    "        cv2.putText(img=ROI, text=id_obj+':'+str(np.round(conf[ix],2)),\n",
    "                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.8, color=(0, 0, 255),thickness=1)\n",
    "       \n",
    "    \n",
    "            \n",
    "    #drawing the number of object\n",
    "    cv2.putText(img=frame, text=f'Counts Objects in ROI: {count_p}', \n",
    "                org= (30,40), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "                fontScale=1.5, color=(255, 0, 0), thickness=1)\n",
    "    \n",
    "    # Filtering tracks history\n",
    "    centers_old = filter_tracks(centers_old, patience)\n",
    "    if verbose:\n",
    "        print(contador_in, contador_out)\n",
    "    \n",
    "    #Drawing the ROI area\n",
    "    overlay = frame.copy()\n",
    "  \n",
    "    cv2.polylines(overlay, pts = area_roi, isClosed = True, color=(255, 0, 0),thickness=2)\n",
    "    cv2.fillPoly(overlay, area_roi, (255,0,0))\n",
    "    frame = cv2.addWeighted(overlay, alpha,frame , 1 - alpha, 0)\n",
    "\n",
    "    #Saving frames in a list \n",
    "    frames_list.append(frame)\n",
    "    #saving transformed frames in a output video formaat\n",
    "    output_video.write(frame)\n",
    "    \n",
    "#Releasing the video    \n",
    "output_video.release()\n",
    "\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pos processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
